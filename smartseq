tar -xzvf /smartseq/mm10_genome.tar.gz
tar -xzvf ./smartseq/mm10_genome.tar.gz -C ./smartseq/
hisat2 --dta -t -x ./smartseq/mm10/genome -1 ./SRR18472352_1.fastq.gz -2 ./SRR18472352_2.fastq.gz -S ./smartseq/hisat/CK4.sam 
#!/bin/bash

# 设置FASTQ文件目录和输出目录
FASTQ_DIR="./"
OUTPUT_DIR="./smartseq/hisat/"
HISAT_INDEX="./smartseq/mm10/genome"

# 遍历FASTQ文件目录中的每个单元格文件
for FASTQ_FILE in ${FASTQ_DIR}*_1.fastq.gz; do
    # 获取不含路径和文件后缀的文件名
    BASE_NAME=$(basename ${FASTQ_FILE} _1.fastq.gz)
    
    # 定义匹配的配对FASTQ文件名
    FASTQ1="${FASTQ_DIR}${BASE_NAME}_1.fastq.gz"
    FASTQ2="${FASTQ_DIR}${BASE_NAME}_2.fastq.gz"
    
    # 定义输出的SAM文件名
    OUTPUT_SAM="${OUTPUT_DIR}${BASE_NAME}.sam"

# 检查SAM文件是否已经存在
    if [ -f "${OUTPUT_SAM}" ]; then
        echo "SAM file ${OUTPUT_SAM} already exists, skipping..."
        continue
    fi
    
    # 运行HISAT2进行比对
    hisat2 -x ${HISAT_INDEX} -1 ${FASTQ1} -2 ${FASTQ2} -S ${OUTPUT_SAM} --dta -t
    
    echo "Processed ${BASE_NAME} and generated ${OUTPUT_SAM}"
done

echo "All files have been processed."


sam转bam：

#!/bin/bash

# 设置SAM文件的输入目录和BAM文件的输出目录
SAM_DIR="./smartseq/hisat/"
BAM_DIR="./smartseq/bam/"

# 遍历输入目录中的每个SAM文件
for SAM_FILE in ${SAM_DIR}*.sam; do
    # 获取不含路径和文件后缀的文件名
    BASE_NAME=$(basename ${SAM_FILE} .sam)
    
    # 定义输出的BAM文件名和排序后的BAM文件名
    OUTPUT_BAM="${BAM_DIR}${BASE_NAME}.bam"
    SORTED_BAM_FILE="${BAM_DIR}${BASE_NAME}_sorted.bam"
    
    # 检查排序后的BAM文件是否已经存在
    if [ -f "${SORTED_BAM_FILE}" ]; then
        echo "Sorted BAM file ${SORTED_BAM_FILE} already exists, skipping..."
        continue
    fi
    
    # 转换SAM为BAM
    echo "Converting ${SAM_FILE} to ${OUTPUT_BAM}..."
    samtools view -S -b ${SAM_FILE} > ${OUTPUT_BAM}

    # 排序BAM文件
    echo "Sorting ${OUTPUT_BAM} to ${SORTED_BAM_FILE}..."
    samtools sort ${OUTPUT_BAM} -o ${SORTED_BAM_FILE}

    # 创建BAM文件索引
    echo "Indexing ${SORTED_BAM_FILE}..."
    samtools index ${SORTED_BAM_FILE}
    
    echo "Converted ${SAM_FILE} to BAM, sorted, and indexed."
done

echo "All SAM files have been converted to sorted and indexed BAM files."

安装feature counts
wget https://jaist.dl.sourceforge.net/project/subread/subread-1.6.2/subread-1.6.2-Linux-x86_64.tar.gz
tar -zxvf  subread-1.6.2-Linux-x86_64.tar.gz
cd subread-1.6.2-Linux-x86_64/bin
echo 'PATH=$PATH:~/subread-1.6.2-Linux-x86_64/bin' >> ~/.bashrc
source ~/.bashrc


#featurecounts运行

#!/bin/bash
# 设置BAM文件目录和输出目录
BAM_DIR="./smartseq/bam/" OUTPUT_DIR="./smartseq/featurecounts/" GTF_FILE="./genomic.gtf"
# 检查BAM目录是否存在
if [ ! -d "$BAM_DIR" ]; then
    echo "Directory $BAM_DIR does not exist."
    exit 1 
fi
# 使用 find 命令找到所有符合条件的 BAM 文件
BAM_FILES=$(find ${BAM_DIR} -name '*_sorted.bam')
# 检查是否找到了 BAM 文件
if [ -z "$BAM_FILES" ]; then
    echo "No BAM files found matching the pattern *_sorted.bam"
    exit 1 
fi
# 运行 featureCounts 对找到的 BAM 文件进行计数
featureCounts \
    -T 8 \
    -p \
    -t exon \
    -g gene_id \
    -a ${GTF_FILE} \
    -o ${OUTPUT_DIR}all_feature.txt \
    ${BAM_FILES}
echo "FeatureCounts processing complete. Results are saved in ${OUTPUT_DIR}all_feature.txt"

删除之前的文件
rm ./smartseq/bam/*_sorted.bam



重新分类索引
#!/bin/bash

# 设置SAM文件的输入目录和BAM文件的输出目录
SAM_DIR="./smartseq/hisat/"
BAM_DIR="./smartseq/bam/"

# 遍历输入目录中的每个SAM文件
for SAM_FILE in ${SAM_DIR}*.sam; do
    # 获取不含路径和文件后缀的文件名
    BASE_NAME=$(basename ${SAM_FILE} .sam)
    
    # 定义输出的BAM文件名和按名称排序后的BAM文件名
    OUTPUT_BAM="${BAM_DIR}${BASE_NAME}.bam"
    SORTED_BAM_FILE="${BAM_DIR}${BASE_NAME}_sorted.bam"

    # 按名称排序BAM文件
    echo "Sorting ${OUTPUT_BAM} by name to ${SORTED_BAM_FILE}..."
    samtools sort -n ${OUTPUT_BAM} -o ${SORTED_BAM_FILE}

    # 强制重新创建BAM文件索引
    echo "Indexing ${SORTED_BAM_FILE}..."
    samtools index ${SORTED_BAM_FILE}
    
    echo "Processed ${SAM_FILE}: Converted to BAM, name-sorted, and indexed."
done

echo "All SAM files have been converted, name-sorted, and indexed."



#featurecounts使用
library(tidyverse)
library(data.table)
setwd("E:/R/smartseq/")
#### 对counts进行处理筛选得到表达矩阵 ####
a1 <- fread('./all_feature.txt',
              header = T,data.table = F)#载入counts，第一列设置为列名
colnames(a1)
counts <- a1[,7:ncol(a1)] #截取样本基因表达量的counts部分作为counts 
rownames(counts) <- a1$Geneid #将基因名作为行名

# 查找以 "_sorted.bam" 结尾的列名
cols_to_remove <- grep("_sorted.bam$", colnames(counts))

# 删除这些列
counts <- counts[, -cols_to_remove]

#更改样品名
colnames(counts)
colnames(counts) <- gsub('./smartseq/1/bam/','', #删除样品名前缀
+                          gsub('.bam','',  colnames(counts))) #删除样品名后缀

metadata <- read.csv('./SraRunTable.txt')  # 替换为你的实际文件路径

# 匹配 counts 的列名和元数据中的样本编号 (Run 列)
metadata_ordered <- metadata[match(colnames(counts), metadata$Run), ]

group_list <- paste(metadata_ordered$AGE, metadata_ordered$source_name, sep = "_")
gl <- data.frame(row.names = colnames(counts), group_list = group_list)

# 替换 postnatal day 30_Wild type cochlear inner hair cells 为 P30_WT IHC
gl$group_list <- gsub("postnatal day 30_Wild type cochlear inner hair cells", "P30_WT IHC", gl$group_list)

# 替换 postnatal day 14_Wild type cochlear inner hair cells 为 P14_WT IHC
gl$group_list <- gsub("postnatal day 14_Wild type cochlear inner hair cells", "P14_WT IHC", gl$group_list)

# 替换 postnatal day 14_induced OHCs (iOHCs) from the cochlear inner hair cells after Tbx2 deletion 为 P14_iOHC
gl$group_list <- gsub("postnatal day 14_induced OHCs \\(iOHCs\\) from the cochlear inner hair cells after Tbx2 deletion", "P14_iOHC", gl$group_list)

# 1. 提取分组信息
group_list <- gl$group_list

# 2. 为每个组生成递增的编号
# ave() 函数生成组内递增编号
sample_counts <- ave(group_list, group_list, FUN = seq_along)

# 3. 拼接分组名和编号，形成新列名
new_colnames <- paste(group_list, sample_counts, sep = "-")

# 4. 替换 counts 矩阵的列名
colnames(counts) <- new_colnames

# 检查新列名是否已经正确替换
head(colnames(counts))

 # 5. 对新列名进行排序，确保同一组的样本排在一起
> sorted_indices <- order(group_list)
> 
> # 6. 重新排序 counts 矩阵的列
> counts_sorted <- counts[, sorted_indices]
# 7. 重新排序 gl 数据框的行，保持与 counts_sorted 一致
gl_sorted <- gl[sorted_indices, ]
第七步没用


##################以下为.sh文件内容
gtf="gencode.vM25.chr_patch_hapl_scaff.annotation.gtf"
### gene_id to gene_name
grep 'gene_id' $gtf | awk -F 'gene_id \"' '{print $2}' |awk -F '\"' '{print $1}' >gene_id_tmp
grep 'gene_id' $gtf | awk -F 'gene_name \"' '{print $2}' |awk -F '\"' '{print $1}' >gene_name_tmp
paste gene_id_tmp gene_name_tmp >last_tmp
uniq last_tmp >g2s_vm25_gencode.txt
rm *_tmp

##python里更换geneid
import pandas as pd

# 读取 counts_sorted 和 g2s
counts_sorted = pd.read_csv('counts_sorted.csv', index_col=0)  # 假设 counts_sorted 是保存为 CSV 文件
g2s = pd.read_csv('g2s_vm20_gencode.txt', sep='\t', header=None, names=['geneid', 'symbol'])

# 匹配 counts_sorted 的 geneid 和 g2s 中的 symbol
counts_sorted['symbol'] = g2s.set_index('geneid').loc[counts_sorted.index, 'symbol']

# 将 geneid 替换为 symbol
counts_sorted.set_index('symbol', inplace=True)

# 导出修改后的 counts_sorted
counts_sorted.to_csv('counts_sorted_with_symbols.csv')

### 从featurecounts 原始输出文件counts.txt中提取Geneid、Length(转录本长度)，计算tpm
geneid_efflen <- subset(a1,select = c("Geneid","Length"))
colnames(geneid_efflen) <- c("geneid","efflen")
View(geneid_efflen)
### 取出counts中geneid对应的efflen
efflen <- geneid_efflen[match(rownames(counts),
geneid_efflen$geneid),
"efflen"]
### 计算 TPM 公式
#TPM (Transcripts Per Kilobase Million)  每千个碱基的转录每百万映射读取的Transcripts
counts2TPM <- function(count=count, efflength=efflen){
RPK <- count/(efflength/1000)   #每千碱基reads (Reads Per Kilobase) 长度标准化
PMSC_rpk <- sum(RPK)/1e6        #RPK的每百万缩放因子 (“per million” scaling factor ) 深度标准化
RPK/PMSC_rpk
}

tpm <- as.data.frame(apply(counts,2,counts2TPM))
colSums(tpm)
View(tpm)
# 将 tpm 数据框导出为 CSV 文件
write.csv(tpm, file = "tpm_output.csv", row.names = TRUE)

# 读取CSV文件（不使用行名）
counts_sorted_with_symbols <- read.csv("counts_sorted_with_symbols.csv", header = TRUE, sep = ",")

# 对重复基因的表达值相加（以 symbol 为基准）
counts_sorted_with_symbols <- aggregate(. ~ symbol, data = counts_sorted_with_symbols, FUN = sum)

# 重新设置 symbol 作为行名
rownames(counts_sorted_with_symbols) <- counts_sorted_with_symbols$symbol
counts_sorted_with_symbols$symbol <- NULL

#导入数据，有重复行怎么办？目前的解法是把行名先放在第一列，然后合并重复的，然后再把第一列放回行名
#已导入数据，开始筛选
# 感兴趣的基因
gene3 <- c("Insm1", "Myo6", "Atoh1")

# 确认基因是否都在 E16 中
gene3 <- gene3[gene3 %in% rownames(E16)]

# 筛选出这些基因对应的行
E16G <- E16[gene3, ]

# 保留这些基因在所有列中都大于0的列
cols_to_keep3 <- colSums(E16G > 0) == length(gene3)

# 筛选列并得到新的数据框
E16F <- E16[, cols_to_keep3]

# 查看结果
head(E16F)

#P1筛选
# 感兴趣的基因
gene4 <- c("Bcl11b", "Myo6", "Myo7a", "Atoh1")

# 确认基因是否都在 P1 中
gene4 <- gene4[gene4 %in% rownames(P1)]

# 筛选出这些基因对应的行
P1G <- P1[gene4, ]

# 保留这些基因在所有列中都大于0的列
cols_to_keep4 <- colSums(P1G > 0) == length(gene4)

# 筛选列并得到新的数据框
P1F <- P1[, cols_to_keep4]

# 查看结果
head(P1F)



# 感兴趣的基因
gene5 <- c("Slc26a5", "Myo6", "Ocm", "Ikzf2")

# 确认基因是否都在 P7 中
gene5 <- gene5[gene5 %in% rownames(P7)]

# 筛选出这些基因对应的行
P7G <- P7[gene5, ]

# 保留这些基因在所有列中都大于0的列
cols_to_keep5 <- colSums(P7G > 0) == length(gene5)

# 筛选列并得到新的数据框
P7F <- P7[, cols_to_keep5]

# 查看结果
head(P7F)

#标准化
library(Seurat)
P14s <- CreateSeuratObject(counts = P14_iOHCF)
P14s <- NormalizeData(object = P14s, normalization.method = "LogNormalize", scale.factor = 10000)
P30s <- CreateSeuratObject(counts = P30)
P30s <- NormalizeData(object = P30s, normalization.method = "LogNormalize", scale.factor = 10000)

#把已经标准化的另外三个矩阵放到seurat的data插槽里边
# 创建 Seurat 对象
E16Fs <- CreateSeuratObject(counts = E16F, project = "E16F")
# 确保将 E16F 转换为 Matrix 类型
E16F_matrix <- as(E16F, "Matrix")
E16Fs <- SetAssayData(object = E16Fs, slot = "data", new.data = E16F_matrix)

P1Fs <- CreateSeuratObject(counts = P1F, project = "P1F")
P1F_matrix <- as(P1F, "Matrix")
P1Fs <- SetAssayData(object = P1Fs, slot = "data", new.data = P1F_matrix) #然后换成P7再来一遍

reference.list <- list(P14s, P30s, E16Fs, P1Fs, P7Fs)
anchors <- FindIntegrationAnchors(object.list = reference.list, anchor.features = 2000, dims = 1:20)
integrated_seurat <- IntegrateData(anchorset = anchors, k.weight = 40) #默认kweight一百，因为细胞数太少了所以减小到40

DefaultAssay(integrated_seurat)
[1] "integrated"
# 提取表达矩阵
expression_matrix <- GetAssayData(integrated_seurat, slot = "data")

# 提取元数据
cell_metadata <- integrated_seurat@meta.data

# 提取特征数据（即基因信息）
gene_metadata <- data.frame(gene_short_name = rownames(expression_matrix))
rownames(gene_metadata) <- rownames(expression_matrix)

cds <- new_cell_data_set(expression_matrix,
                          cell_metadata = cell_metadata,
                          gene_metadata = gene_metadata)

# 假设你已经有不同矩阵的列名
cell_names_P1 <- colnames(P1F)
cell_names_P7 <- colnames(P7F)
cell_names_P14 <- colnames(P14_iOHCF)
cell_names_E16 <- colnames(E16F)
cell_names_P30 <- colnames(P30)

# 为每个细胞类型创建一个元数据框
metadata <- data.frame(row.names = colnames(cds),
                       cell_type = rep(NA, ncol(cds)))

# 为每个矩阵分配细胞类型
metadata[cell_names_P1, "cell_type"] <- "P1"
metadata[cell_names_P7, "cell_type"] <- "P7"
metadata[cell_names_P14, "cell_type"] <- "P14"
metadata[cell_names_E16, "cell_type"] <- "E16"
metadata[cell_names_P30, "cell_type"] <- "P30"

# 将元数据添加到cds对象中
colData(cds)$cell_type <- metadata$cell_type

# 检查cell_type列
table(colData(cds)$cell_type)

cds <- preprocess_cds(cds, num_dim = 50, norm_method = "none")

#主成分可视化
plot_pc_variance_explained(cds)

cds <- reduce_dimension(cds,
                        core = 8, 
                        umap.fast_sgd = F)（设置成T可加速）
