tar -xzvf /smartseq/mm10_genome.tar.gz
tar -xzvf ./smartseq/mm10_genome.tar.gz -C ./smartseq/
hisat2 --dta -t -x ./smartseq/mm10/genome -1 ./SRR18472352_1.fastq.gz -2 ./SRR18472352_2.fastq.gz -S ./smartseq/hisat/CK4.sam 
#!/bin/bash

# 设置FASTQ文件目录和输出目录
FASTQ_DIR="./"
OUTPUT_DIR="./smartseq/hisat/"
HISAT_INDEX="./smartseq/mm10/genome"

# 遍历FASTQ文件目录中的每个单元格文件
for FASTQ_FILE in ${FASTQ_DIR}*_1.fastq.gz; do
    # 获取不含路径和文件后缀的文件名
    BASE_NAME=$(basename ${FASTQ_FILE} _1.fastq.gz)
    
    # 定义匹配的配对FASTQ文件名
    FASTQ1="${FASTQ_DIR}${BASE_NAME}_1.fastq.gz"
    FASTQ2="${FASTQ_DIR}${BASE_NAME}_2.fastq.gz"
    
    # 定义输出的SAM文件名
    OUTPUT_SAM="${OUTPUT_DIR}${BASE_NAME}.sam"

# 检查SAM文件是否已经存在
    if [ -f "${OUTPUT_SAM}" ]; then
        echo "SAM file ${OUTPUT_SAM} already exists, skipping..."
        continue
    fi
    
    # 运行HISAT2进行比对
    hisat2 -x ${HISAT_INDEX} -1 ${FASTQ1} -2 ${FASTQ2} -S ${OUTPUT_SAM} --dta -t
    
    echo "Processed ${BASE_NAME} and generated ${OUTPUT_SAM}"
done

echo "All files have been processed."


sam转bam：

#!/bin/bash

# 设置SAM文件的输入目录和BAM文件的输出目录
SAM_DIR="./smartseq/hisat/"
BAM_DIR="./smartseq/bam/"

# 遍历输入目录中的每个SAM文件
for SAM_FILE in ${SAM_DIR}*.sam; do
    # 获取不含路径和文件后缀的文件名
    BASE_NAME=$(basename ${SAM_FILE} .sam)
    
    # 定义输出的BAM文件名和排序后的BAM文件名
    OUTPUT_BAM="${BAM_DIR}${BASE_NAME}.bam"
    SORTED_BAM_FILE="${BAM_DIR}${BASE_NAME}_sorted.bam"
    
    # 检查排序后的BAM文件是否已经存在
    if [ -f "${SORTED_BAM_FILE}" ]; then
        echo "Sorted BAM file ${SORTED_BAM_FILE} already exists, skipping..."
        continue
    fi
    
    # 转换SAM为BAM
    echo "Converting ${SAM_FILE} to ${OUTPUT_BAM}..."
    samtools view -S -b ${SAM_FILE} > ${OUTPUT_BAM}

    # 排序BAM文件
    echo "Sorting ${OUTPUT_BAM} to ${SORTED_BAM_FILE}..."
    samtools sort ${OUTPUT_BAM} -o ${SORTED_BAM_FILE}

    # 创建BAM文件索引
    echo "Indexing ${SORTED_BAM_FILE}..."
    samtools index ${SORTED_BAM_FILE}
    
    echo "Converted ${SAM_FILE} to BAM, sorted, and indexed."
done

echo "All SAM files have been converted to sorted and indexed BAM files."

安装feature counts
wget https://jaist.dl.sourceforge.net/project/subread/subread-1.6.2/subread-1.6.2-Linux-x86_64.tar.gz
tar -zxvf  subread-1.6.2-Linux-x86_64.tar.gz
cd subread-1.6.2-Linux-x86_64/bin
echo 'PATH=$PATH:~/subread-1.6.2-Linux-x86_64/bin' >> ~/.bashrc
source ~/.bashrc


#featurecounts运行

#!/bin/bash
# 设置BAM文件目录和输出目录
BAM_DIR="./smartseq/bam/" OUTPUT_DIR="./smartseq/featurecounts/" GTF_FILE="./genomic.gtf"
# 检查BAM目录是否存在
if [ ! -d "$BAM_DIR" ]; then
    echo "Directory $BAM_DIR does not exist."
    exit 1 
fi
# 使用 find 命令找到所有符合条件的 BAM 文件
BAM_FILES=$(find ${BAM_DIR} -name '*_sorted.bam')
# 检查是否找到了 BAM 文件
if [ -z "$BAM_FILES" ]; then
    echo "No BAM files found matching the pattern *_sorted.bam"
    exit 1 
fi
# 运行 featureCounts 对找到的 BAM 文件进行计数
featureCounts \
    -T 8 \
    -p \
    -t exon \
    -g gene_id \
    -a ${GTF_FILE} \
    -o ${OUTPUT_DIR}all_feature.txt \
    ${BAM_FILES}
echo "FeatureCounts processing complete. Results are saved in ${OUTPUT_DIR}all_feature.txt"

删除之前的文件
rm ./smartseq/bam/*_sorted.bam



重新分类索引
#!/bin/bash

# 设置SAM文件的输入目录和BAM文件的输出目录
SAM_DIR="./smartseq/hisat/"
BAM_DIR="./smartseq/bam/"

# 遍历输入目录中的每个SAM文件
for SAM_FILE in ${SAM_DIR}*.sam; do
    # 获取不含路径和文件后缀的文件名
    BASE_NAME=$(basename ${SAM_FILE} .sam)
    
    # 定义输出的BAM文件名和按名称排序后的BAM文件名
    OUTPUT_BAM="${BAM_DIR}${BASE_NAME}.bam"
    SORTED_BAM_FILE="${BAM_DIR}${BASE_NAME}_sorted.bam"

    # 按名称排序BAM文件
    echo "Sorting ${OUTPUT_BAM} by name to ${SORTED_BAM_FILE}..."
    samtools sort -n ${OUTPUT_BAM} -o ${SORTED_BAM_FILE}

    # 强制重新创建BAM文件索引
    echo "Indexing ${SORTED_BAM_FILE}..."
    samtools index ${SORTED_BAM_FILE}
    
    echo "Processed ${SAM_FILE}: Converted to BAM, name-sorted, and indexed."
done

echo "All SAM files have been converted, name-sorted, and indexed."



#featurecounts使用
library(tidyverse)
library(data.table)
setwd("E:/R/smartseq/")
#### 对counts进行处理筛选得到表达矩阵 ####
a1 <- fread('./all_feature.txt',
              header = T,data.table = F)#载入counts，第一列设置为列名
colnames(a1)
counts <- a1[,7:ncol(a1)] #截取样本基因表达量的counts部分作为counts 
rownames(counts) <- a1$Geneid #将基因名作为行名

# 查找以 "_sorted.bam" 结尾的列名
cols_to_remove <- grep("_sorted.bam$", colnames(counts))

# 删除这些列
counts <- counts[, -cols_to_remove]

#更改样品名
colnames(counts)
colnames(counts) <- gsub('./smartseq/1/bam/','', #删除样品名前缀
+                          gsub('.bam','',  colnames(counts))) #删除样品名后缀

metadata <- read.csv('./SraRunTable.txt')  # 替换为你的实际文件路径

# 匹配 counts 的列名和元数据中的样本编号 (Run 列)
metadata_ordered <- metadata[match(colnames(counts), metadata$Run), ]

group_list <- paste(metadata_ordered$AGE, metadata_ordered$source_name, sep = "_")
gl <- data.frame(row.names = colnames(counts), group_list = group_list)

# 替换 postnatal day 30_Wild type cochlear inner hair cells 为 P30_WT IHC
gl$group_list <- gsub("postnatal day 30_Wild type cochlear inner hair cells", "P30_WT IHC", gl$group_list)

# 替换 postnatal day 14_Wild type cochlear inner hair cells 为 P14_WT IHC
gl$group_list <- gsub("postnatal day 14_Wild type cochlear inner hair cells", "P14_WT IHC", gl$group_list)

# 替换 postnatal day 14_induced OHCs (iOHCs) from the cochlear inner hair cells after Tbx2 deletion 为 P14_iOHC
gl$group_list <- gsub("postnatal day 14_induced OHCs \\(iOHCs\\) from the cochlear inner hair cells after Tbx2 deletion", "P14_iOHC", gl$group_list)

# 1. 提取分组信息
group_list <- gl$group_list

# 2. 为每个组生成递增的编号
# ave() 函数生成组内递增编号
sample_counts <- ave(group_list, group_list, FUN = seq_along)

# 3. 拼接分组名和编号，形成新列名
new_colnames <- paste(group_list, sample_counts, sep = "-")

# 4. 替换 counts 矩阵的列名
colnames(counts) <- new_colnames

# 检查新列名是否已经正确替换
head(colnames(counts))

 # 5. 对新列名进行排序，确保同一组的样本排在一起
> sorted_indices <- order(group_list)
> 
> # 6. 重新排序 counts 矩阵的列
> counts_sorted <- counts[, sorted_indices]
# 7. 重新排序 gl 数据框的行，保持与 counts_sorted 一致
gl_sorted <- gl[sorted_indices, ]
第七步没用


##################以下为.sh文件内容
gtf="gencode.vM25.chr_patch_hapl_scaff.annotation.gtf"
### gene_id to gene_name
grep 'gene_id' $gtf | awk -F 'gene_id \"' '{print $2}' |awk -F '\"' '{print $1}' >gene_id_tmp
grep 'gene_id' $gtf | awk -F 'gene_name \"' '{print $2}' |awk -F '\"' '{print $1}' >gene_name_tmp
paste gene_id_tmp gene_name_tmp >last_tmp
uniq last_tmp >g2s_vm25_gencode.txt
rm *_tmp

##python里更换geneid
import pandas as pd

# 读取 counts_sorted 和 g2s
counts_sorted = pd.read_csv('counts_sorted.csv', index_col=0)  # 假设 counts_sorted 是保存为 CSV 文件
g2s = pd.read_csv('g2s_vm20_gencode.txt', sep='\t', header=None, names=['geneid', 'symbol'])

# 匹配 counts_sorted 的 geneid 和 g2s 中的 symbol
counts_sorted['symbol'] = g2s.set_index('geneid').loc[counts_sorted.index, 'symbol']

# 将 geneid 替换为 symbol
counts_sorted.set_index('symbol', inplace=True)

# 导出修改后的 counts_sorted
counts_sorted.to_csv('counts_sorted_with_symbols.csv')

### 从featurecounts 原始输出文件counts.txt中提取Geneid、Length(转录本长度)，计算tpm
geneid_efflen <- subset(a1,select = c("Geneid","Length"))
colnames(geneid_efflen) <- c("geneid","efflen")
View(geneid_efflen)
### 取出counts中geneid对应的efflen
efflen <- geneid_efflen[match(rownames(counts),
geneid_efflen$geneid),
"efflen"]
### 计算 TPM 公式
#TPM (Transcripts Per Kilobase Million)  每千个碱基的转录每百万映射读取的Transcripts
counts2TPM <- function(count=count, efflength=efflen){
RPK <- count/(efflength/1000)   #每千碱基reads (Reads Per Kilobase) 长度标准化
PMSC_rpk <- sum(RPK)/1e6        #RPK的每百万缩放因子 (“per million” scaling factor ) 深度标准化
RPK/PMSC_rpk
}

tpm <- as.data.frame(apply(counts,2,counts2TPM))
colSums(tpm)
View(tpm)
# 将 tpm 数据框导出为 CSV 文件
write.csv(tpm, file = "tpm_output.csv", row.names = TRUE)
